
@article{higdonComputerModelCalibration2008,
	title = {Computer Model Calibration Using High-Dimensional Output},
	volume = {103},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214507000000888},
	doi = {10.1198/016214507000000888},
	abstract = {This work focuses on combining observations from field experiments with detailed computer simulations of a physical process to carry out statistical inference. Of particular interest here is determining uncertainty in resulting predictions. This typically involves calibration of parameters in the computer simulator as well as accounting for inadequate physics in the simulator. The problem is complicated by the fact that simulation code is sufficiently demanding that only a limited number of simulations can be carried out. We consider applications in characterizing material properties for which the field data and the simulator output are highly multivariate. For example, the experimental data and simulation output may be an image or may describe the shape of a physical object. We make use of the basic framework of Kennedy and O'Hagan. However, the size and multivariate nature of the data lead to computational challenges in implementing the framework. To overcome these challenges, we make use of basis representations (e.g., principal components) to reduce the dimensionality of the problem and speed up the computations required for exploring the posterior distribution. This methodology is applied to applications, both ongoing and historical, at Los Alamos National Laboratory.},
	pages = {570--583},
	number = {482},
	journaltitle = {Journal of the American Statistical Association},
	author = {Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
	urldate = {2020-09-15},
	date = {2008-06-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214507000000888},
	keywords = {Computer experiments, Functional data analysis, Gaussian process, Prediction, Predictive science, Uncertainty quantification},
	file = {Full Text PDF:/home/enitex/Zotero/storage/DJ3EGIVH/Higdon et al. - 2008 - Computer Model Calibration Using High-Dimensional .pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/HLJDG8L9/016214507000000888.html:text/html}
}

@article{pratolaBayesianAdditiveRegression2016,
	title = {Bayesian Additive Regression Tree Calibration of Complex High-Dimensional Computer Models},
	volume = {58},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.2015.1049749},
	doi = {10.1080/00401706.2015.1049749},
	abstract = {Complex natural phenomena are increasingly investigated by the use of a complex computer simulator. To leverage the advantages of simulators, observational data need to be incorporated in a probabilistic framework so that uncertainties can be quantified. A popular framework for such experiments is the statistical computer model calibration experiment. A limitation often encountered in current statistical approaches for such experiments is the difficulty in modeling high-dimensional observational datasets and simulator outputs as well as high-dimensional inputs. As the complexity of simulators seems to only grow, this challenge will continue unabated. In this article, we develop a Bayesian statistical calibration approach that is ideally suited for such challenging calibration problems. Our approach leverages recent ideas from Bayesian additive regression Tree models to construct a random basis representation of the simulator outputs and observational data. The approach can flexibly handle high-dimensional datasets, high-dimensional simulator inputs, and calibration parameters while quantifying important sources of uncertainty in the resulting inference. We demonstrate our methodology on a {CO}2 emissions rate calibration problem, and on a complex simulator of subterranean radionuclide dispersion, which simulates the spatial–temporal diffusion of radionuclides released during nuclear bomb tests at the Nevada Test Site. Supplementary computer code and datasets are available online.},
	pages = {166--179},
	number = {2},
	journaltitle = {Technometrics},
	author = {Pratola, M. T. and Higdon, D. M.},
	urldate = {2020-09-15},
	date = {2016-04-02},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2015.1049749},
	keywords = {Uncertainty quantification, Catastrophe model, Climate change, Markov chain Monte Carlo, Nonparametric, Treaty verification},
	file = {Full Text PDF:/home/enitex/Zotero/storage/MV66QGTP/Pratola and Higdon - 2016 - Bayesian Additive Regression Tree Calibration of C.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/8I2WTDQA/00401706.2015.html:text/html}
}

@article{salterEfficientCalibrationHighdimensional2019,
	title = {Efficient calibration for high-dimensional computer model output using basis methods},
	url = {http://arxiv.org/abs/1906.05758},
	abstract = {Calibration of expensive computer models with high-dimensional output fields can be approached via history matching. If the entire output field is matched, with patterns or correlations between locations or time points represented, calculating the distance metric between observational data and model output for a single input setting requires a time intensive inversion of a high-dimensional matrix. By using a low-dimensional basis representation rather than emulating each output individually, we define a metric in the reduced space that allows the implausibility for the field to be calculated efficiently, with only small matrix inversions required, using projection that is consistent with the variance specifications in the implausibility. We show that projection using the \$L\_2\$ norm can result in different conclusions, with the ordering of points not maintained on the basis, with implications for both history matching and probabilistic methods. We demonstrate the scalability of our method through history matching of the Canadian atmosphere model, {CanAM}4, comparing basis methods to emulation of each output individually, showing that the basis approach can be more accurate, whilst also being more efficient.},
	journaltitle = {{arXiv}:1906.05758 [stat]},
	author = {Salter, James M. and Williamson, Daniel B.},
	urldate = {2020-09-15},
	date = {2019-06-13},
	eprinttype = {arxiv},
	eprint = {1906.05758},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/enitex/Zotero/storage/L5VM55LQ/Salter and Williamson - 2019 - Efficient calibration for high-dimensional compute.pdf:application/pdf;arXiv.org Snapshot:/home/enitex/Zotero/storage/QIND66MU/1906.html:text/html}
}

@article{loeppkyChoosingSampleSize2009,
	title = {Choosing the Sample Size of a Computer Experiment: A Practical Guide},
	volume = {51},
	issn = {0040-1706},
	url = {https://amstat.tandfonline.com/doi/abs/10.1198/TECH.2009.08040},
	doi = {10.1198/TECH.2009.08040},
	shorttitle = {Choosing the Sample Size of a Computer Experiment},
	abstract = {We provide reasons and evidence supporting the informal rule that the number of runs for an effective initial computer experiment should be about 10 times the input dimension. Our arguments quantify two key characteristics of computer codes that affect the sample size required for a desired level of accuracy when approximating the code via a Gaussian process ({GP}). The first characteristic is the total sensitivity of a code output variable to all input variables; the second corresponds to the way this total sensitivity is distributed across the input variables, specifically the possible presence of a few prominent input factors and many impotent ones (i.e., effect sparsity). Both measures relate directly to the correlation structure in the {GP} approximation of the code. In this way, the article moves toward a more formal treatment of sample size for a computer experiment. The evidence supporting these arguments stems primarily from a simulation study and via specific codes modeling climate and ligand activation of G-protein.},
	pages = {366--376},
	number = {4},
	journaltitle = {Technometrics},
	shortjournal = {null},
	author = {Loeppky, Jason L. and Sacks, Jerome and Welch, William J.},
	urldate = {2020-09-15},
	date = {2009-11-01},
	note = {Publisher: Taylor \& Francis},
	file = {Snapshot:/home/enitex/Zotero/storage/3J2YH4Z3/TECH.2009.html:text/html;Submitted Version:/home/enitex/Zotero/storage/3WBX86G8/Loeppky et al. - 2009 - Choosing the Sample Size of a Computer Experiment.pdf:application/pdf}
}

@article{bayarriFrameworkValidationComputer2007,
	title = {A Framework for Validation of Computer Models},
	volume = {49},
	issn = {0040-1706},
	url = {https://doi.org/10.1198/004017007000000092},
	doi = {10.1198/004017007000000092},
	abstract = {We present a framework that enables computer model evaluation oriented toward answering the question: Does the computer model adequately represent reality? The proposed validation framework is a six-step procedure based on Bayesian and likelihood methodology. The Bayesian methodology is particularly well suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models, combining multiple sources of information, and updating validation assessments as new information is acquired. Moreover, it allows inferential statements to be made about predictive error associated with model predictions in untested situations. The framework is implemented in a test bed example of resistance spot welding, to provide context for each of the six steps in the proposed validation process.},
	pages = {138--154},
	number = {2},
	journaltitle = {Technometrics},
	author = {Bayarri, Maria J. and Berger, James O. and Paulo, Rui and Sacks, Jerry and Cafeo, John A. and Cavendish, James and Lin, Chin-Hsu and Tu, Jian},
	urldate = {2020-09-15},
	date = {2007-05-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/004017007000000092},
	keywords = {Prediction, Bayesian analysis, Identifiability, Model discrepancy},
	file = {Full Text PDF:/home/enitex/Zotero/storage/FI8AKUGH/Bayarri et al. - 2007 - A Framework for Validation of Computer Models.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/KSUVIKBF/004017007000000092.html:text/html}
}

@article{loeppkyChoosingSampleSize2009a,
	title = {Choosing the Sample Size of a Computer Experiment: A Practical Guide},
	volume = {51},
	issn = {0040-1706},
	url = {https://amstat.tandfonline.com/doi/abs/10.1198/TECH.2009.08040},
	doi = {10.1198/TECH.2009.08040},
	shorttitle = {Choosing the Sample Size of a Computer Experiment},
	abstract = {We provide reasons and evidence supporting the informal rule that the number of runs for an effective initial computer experiment should be about 10 times the input dimension. Our arguments quantify two key characteristics of computer codes that affect the sample size required for a desired level of accuracy when approximating the code via a Gaussian process ({GP}). The first characteristic is the total sensitivity of a code output variable to all input variables; the second corresponds to the way this total sensitivity is distributed across the input variables, specifically the possible presence of a few prominent input factors and many impotent ones (i.e., effect sparsity). Both measures relate directly to the correlation structure in the {GP} approximation of the code. In this way, the article moves toward a more formal treatment of sample size for a computer experiment. The evidence supporting these arguments stems primarily from a simulation study and via specific codes modeling climate and ligand activation of G-protein.},
	pages = {366--376},
	number = {4},
	journaltitle = {Technometrics},
	shortjournal = {null},
	author = {Loeppky, Jason L. and Sacks, Jerome and Welch, William J.},
	urldate = {2020-09-15},
	date = {2009-11-01},
	note = {Publisher: Taylor \& Francis},
	file = {Snapshot:/home/enitex/Zotero/storage/X5AJ6E4T/TECH.2009.html:text/html;Submitted Version:/home/enitex/Zotero/storage/NQ6TJ56G/Loeppky et al. - 2009 - Choosing the Sample Size of a Computer Experiment.pdf:application/pdf}
}

@article{contiBayesianEmulationComplex2010,
	title = {Bayesian emulation of complex multi-output and dynamic computer models},
	volume = {140},
	issn = {0378-3758},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375809002559},
	doi = {10.1016/j.jspi.2009.08.006},
	abstract = {Computer models are widely used in scientific research to study and predict the behaviour of complex systems. The run times of computer-intensive simulators are often such that it is impractical to make the thousands of model runs that are conventionally required for sensitivity analysis, uncertainty analysis or calibration. In response to this problem, highly efficient techniques have recently been developed based on a statistical meta-model (the emulator) that is built to approximate the computer model. The approach, however, is less straightforward for dynamic simulators, designed to represent time-evolving systems. Generalisations of the established methodology to allow for dynamic emulation are here proposed and contrasted. Advantages and difficulties are discussed and illustrated with an application to the Sheffield Dynamic Global Vegetation Model, developed within the {UK} Centre for Terrestrial Carbon Dynamics.},
	pages = {640--651},
	number = {3},
	journaltitle = {Journal of Statistical Planning and Inference},
	shortjournal = {Journal of Statistical Planning and Inference},
	author = {Conti, Stefano and O’Hagan, Anthony},
	urldate = {2020-09-15},
	date = {2010-03-01},
	langid = {english},
	keywords = {Computer experiments, Bayesian inference, Dynamic models, Hierarchical models},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/KPU8XNDS/S0378375809002559.html:text/html}
}

@article{rougierEfficientEmulatorsMultivariate2008,
	title = {Efficient Emulators for Multivariate Deterministic Functions},
	volume = {17},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/106186008X384032},
	doi = {10.1198/106186008X384032},
	abstract = {One of the challenges with emulating the response of a multivariate function to its inputs is the quantity of data that must be assimilated, which is the product of the number of model evaluations and the number of outputs. This article shows how even large calculations can be made tractable. It is already appreciated that gains can be made when the emulator residual covariance function is treated as separable in the model-inputs and model-outputs. Here, an additional simplification on the structure of the regressors in the emulator mean function allows very substantial further gains. The result is that it is now possible to emulate rapidly—on a desktop computer—models with hundreds of evaluations and hundreds of outputs. This is demonstrated through calculating costs in floating-point operations, and in an illustration. Even larger sets of outputs are possible if they have additional structure, for example, spatial-temporal.},
	pages = {827--843},
	number = {4},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Rougier, Jonathan},
	urldate = {2020-09-15},
	date = {2008-12-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/106186008X384032},
	keywords = {Gaussian process, Kronecker product, Outer-product emulator, Separable variance},
	file = {Full Text PDF:/home/enitex/Zotero/storage/VBVEWDHF/Rougier - 2008 - Efficient Emulators for Multivariate Deterministic.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/IXIXFGYH/106186008X384032.html:text/html}
}

@article{higdonBayesianCalibrationApproach2008,
	title = {A Bayesian calibration approach to the thermal problem},
	volume = {197},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782507005087},
	doi = {10.1016/j.cma.2007.05.031},
	series = {Validation Challenge Workshop},
	abstract = {Many of the problems we work with at Los Alamos National Laboratory are similar to the thermal problem described in the tasking document. In this paper, we describe the tools and methods we have developed that utilize experimental data and detailed physics simulations for uncertainty quantification, and apply them to the thermal challenge problem. We then go on to address the regulatory question posed in the problem description. This statistical framework used here is largely based on the approach of Kennedy and O’Hagan [Kennedy, M., O’Hagan, A., Bayesian calibration of computer models (with discussion), J. Royal Stat. Soc. B 68 (2001) 425–464], but has been extended to deal with functional output of the simulation model.},
	pages = {2431--2441},
	number = {29},
	journaltitle = {Computer Methods in Applied Mechanics and Engineering},
	shortjournal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Higdon, Dave and Nakhleh, Charles and Gattiker, James and Williams, Brian},
	urldate = {2020-09-15},
	date = {2008-05-01},
	langid = {english},
	keywords = {Computer experiments, Functional data analysis, Gaussian process, Predictive science, Uncertainty quantification, Certification, Predictability, Verification and validation},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/VZ625RHR/S0045782507005087.html:text/html;Submitted Version:/home/enitex/Zotero/storage/SMZI339E/Higdon et al. - 2008 - A Bayesian calibration approach to the thermal pro.pdf:application/pdf}
}

@article{frickerMultivariateGaussianProcess2013,
	title = {Multivariate Gaussian Process Emulators With Nonseparable Covariance Structures},
	volume = {55},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.2012.715835},
	doi = {10.1080/00401706.2012.715835},
	abstract = {The Gaussian process regression model is a popular type of “emulator” used as a fast surrogate for computationally expensive simulators (deterministic computer models). For simulators with multivariate output, common practice is to specify a separable covariance structure for the Gaussian process. Though computationally convenient, this can be too restrictive, leading to poor performance of the emulator, particularly when the different simulator outputs represent different physical quantities. Also, treating the simulator outputs as independent can lead to inappropriate representations of joint uncertainty. We develop nonseparable covariance structures for Gaussian process emulators, based on the linear model of coregionalization and convolution methods. Using two case studies, we compare the performance of these covariance structures both with standard separable covariance structures and with emulators that assume independence between the outputs. In each case study, we find that only emulators with nonseparable covariances structures have sufficient flexibility both to give good predictions and to represent joint uncertainty about the simulator outputs appropriately. This article has supplementary material online.},
	pages = {47--56},
	number = {1},
	journaltitle = {Technometrics},
	author = {Fricker, Thomas E. and Oakley, Jeremy E. and Urban, Nathan M.},
	urldate = {2020-09-15},
	date = {2013-02-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2012.715835},
	keywords = {Computer experiment, Convolved process, Coregionalization, Metamodel},
	file = {Full Text PDF:/home/enitex/Zotero/storage/2UPSW5BW/Fricker et al. - 2013 - Multivariate Gaussian Process Emulators With Nonse.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/8TSQ6MV7/00401706.2012.html:text/html}
}

@article{bilionisMultioutputSeparableGaussian2013,
	title = {Multi-output separable Gaussian process: Towards an efficient, fully Bayesian paradigm for uncertainty quantification},
	volume = {241},
	issn = {0021-9991},
	url = {http://www.sciencedirect.com/science/article/pii/S0021999113000417},
	doi = {10.1016/j.jcp.2013.01.011},
	shorttitle = {Multi-output separable Gaussian process},
	abstract = {Computer codes simulating physical systems usually have responses that consist of a set of distinct outputs (e.g., velocity and pressure) that evolve also in space and time and depend on many unknown input parameters (e.g., physical constants, initial/boundary conditions, etc.). Furthermore, essential engineering procedures such as uncertainty quantification, inverse problems or design are notoriously difficult to carry out mostly due to the limited simulations available. The aim of this work is to introduce a fully Bayesian approach for treating these problems which accounts for the uncertainty induced by the finite number of observations. Our model is built on a multi-dimensional Gaussian process that explicitly treats correlations between distinct output variables as well as space and/or time. The proper use of a separable covariance function enables us to describe the huge covariance matrix as a Kronecker product of smaller matrices leading to efficient algorithms for carrying out inference and predictions. The novelty of this work, is the recognition that the Gaussian process model defines a posterior probability measure on the function space of possible surrogates for the computer code and the derivation of an algorithmic procedure that allows us to sample it efficiently. We demonstrate how the scheme can be used in uncertainty quantification tasks in order to obtain error bars for the statistics of interest that account for the finite number of observations.},
	pages = {212--239},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Bilionis, Ilias and Zabaras, Nicholas and Konomi, Bledar A. and Lin, Guang},
	urldate = {2020-09-15},
	date = {2013-05-15},
	langid = {english},
	keywords = {Gaussian process, Uncertainty quantification, Kronecker product, Bayesian, Separable covariance function, Stochastic partial differential equations, Surrogate models},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/W256XLGY/S0021999113000417.html:text/html}
}

@article{bilionisMultioutputLocalGaussian2012,
	title = {Multi-output local Gaussian process regression: Applications to uncertainty quantification},
	volume = {231},
	issn = {0021-9991},
	url = {http://www.sciencedirect.com/science/article/pii/S0021999112002513},
	doi = {10.1016/j.jcp.2012.04.047},
	shorttitle = {Multi-output local Gaussian process regression},
	abstract = {We develop an efficient, Bayesian Uncertainty Quantification framework using a novel treed Gaussian process model. The tree is adaptively constructed using information conveyed by the observed data about the length scales of the underlying process. On each leaf of the tree, we utilize Bayesian Experimental Design techniques in order to learn a multi-output Gaussian process. The constructed surrogate can provide analytical point estimates, as well as error bars, for the statistics of interest. We numerically demonstrate the effectiveness of the suggested framework in identifying discontinuities, local features and unimportant dimensions in the solution of stochastic differential equations.},
	pages = {5718--5746},
	number = {17},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Bilionis, Ilias and Zabaras, Nicholas},
	urldate = {2020-09-15},
	date = {2012-07-01},
	langid = {english},
	keywords = {Gaussian process, Uncertainty quantification, Bayesian, Stochastic partial differential equations, Adaptivity, Multi-element, Multi-output},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/87NNVV5N/S0021999112002513.html:text/html;Bilionis and Zabaras - 2012 - Multi-output local Gaussian process regression Ap.pdf:/home/enitex/Zotero/storage/YX4W3XW5/Bilionis and Zabaras - 2012 - Multi-output local Gaussian process regression Ap.pdf:application/pdf}
}

@article{arendtImprovingIdentifiabilityModel2012,
	title = {Improving Identifiability in Model Calibration Using Multiple Responses},
	volume = {134},
	issn = {1050-0472},
	url = {https://asmedigitalcollection.asme.org/mechanicaldesign/article/134/10/100909/467354/Improving-Identifiability-in-Model-Calibration},
	doi = {10.1115/1.4007573},
	number = {10},
	journaltitle = {Journal of Mechanical Design},
	shortjournal = {J. Mech. Des},
	author = {Arendt, Paul D. and Apley, Daniel W. and Chen, Wei and Lamb, David and Gorsich, David},
	urldate = {2020-09-15},
	date = {2012-10-01},
	langid = {english},
	note = {Publisher: American Society of Mechanical Engineers Digital Collection},
	file = {Snapshot:/home/enitex/Zotero/storage/QABWYJHB/467354.html:text/html;Arendt et al. - 2012 - Improving Identifiability in Model Calibration Usi.pdf:/home/enitex/Zotero/storage/B33YW9T6/Arendt et al. - 2012 - Improving Identifiability in Model Calibration Usi.pdf:application/pdf}
}

@incollection{santnerPhysicalExperimentsComputer2018,
	location = {New York, {NY}},
	title = {Physical Experiments and Computer Experiments},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_1},
	series = {Springer Series in Statistics},
	abstract = {Experiments have long been used to study the relationship between a set of inputs to a physical system and the resulting output. Termed physical experiments in this text, there is a growing trend to replace or supplement the physical system used in such an experiment with a deterministic simulator.},
	pages = {1--26},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_1},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/GK73D3XE/Santner et al. - 2018 - Physical Experiments and Computer Experiments.pdf:application/pdf}
}

@incollection{santnerStochasticProcessModels2018,
	location = {New York, {NY}},
	title = {Stochastic Process Models for Describing Computer Simulator Output},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_2},
	series = {Springer Series in Statistics},
	abstract = {Recall from Chap. 1 that {\textbackslash}({\textbackslash}boldsymbol\{x\}{\textbackslash}) denotes a generic input to our computer simulator and {\textbackslash}(y({\textbackslash}boldsymbol\{x\}){\textbackslash}) denotes the associated output. This chapter will introduce several classes of random function models for {\textbackslash}(y({\textbackslash}boldsymbol\{x\}){\textbackslash}) that will serve as the core building blocks for the interpolators, experimental designs, calibration, and tuning methodologies that will be introduced in later chapters. The reason that the random function approach is so useful is that accurate prediction based on black box computer simulator output requires a rich class of {\textbackslash}(y({\textbackslash}boldsymbol\{x\}){\textbackslash}) options when only a minimal amount might be known about the output function. Indeed, regression mean modeling of simulator output is usually based on a rather arbitrarily selected parametric form.},
	pages = {27--66},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_2},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/MLBZSPKS/Santner et al. - 2018 - Stochastic Process Models for Describing Computer .pdf:application/pdf}
}

@incollection{santnerBayesianInferenceSimulator2018,
	location = {New York, {NY}},
	title = {Bayesian Inference for Simulator Output},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_4},
	series = {Springer Series in Statistics},
	abstract = {In Chap. 3 the correlation and precision parameters are completely unknown for the process model assumed to generate simulator output. In contrast this chapter assumes that the researcher has prior knowledge about the unknown parameters that is quantifiable in the form of a prior distribution.},
	pages = {115--143},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_4},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/8LY4F3FS/Santner et al. - 2018 - Bayesian Inference for Simulator Output.pdf:application/pdf}
}

@incollection{santnerEmpiricalBestLinear2018,
	location = {New York, {NY}},
	title = {Empirical Best Linear Unbiased Prediction of Computer Simulator Output},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_3},
	series = {Springer Series in Statistics},
	abstract = {This chapter and Chap. 4 discuss techniques for predicting output for a computer simulator based on “training” runs from the model. Knowing how to predict computer output is a prerequisite for answering most practical research questions that involve computer simulators including those listed in Sect. 1.3. As an example where the prediction methods described below will be central, Chap. 6 will present a sequential design for a computer experiment to find input conditions {\textbackslash}({\textbackslash}boldsymbol\{x\}{\textbackslash}) that maximize a computer output which requires prediction of {\textbackslash}(y({\textbackslash}boldsymbol\{x\}){\textbackslash}) at all untried sites.},
	pages = {67--114},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_3},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/LU5YNSAP/Santner et al. - 2018 - Empirical Best Linear Unbiased Prediction of Compu.pdf:application/pdf}
}

@incollection{santnerSpaceFillingDesignsComputer2018,
	location = {New York, {NY}},
	title = {Space-Filling Designs for Computer Experiments},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_5},
	series = {Springer Series in Statistics},
	abstract = {This chapter and the next discuss how to select inputs at which to compute the output of a computer experiment to achieve specific goals. The inputs one selects constitute the “experimental design.” As in previous chapters, the inputs are referred to as “runs.” The region corresponding to the values of the inputs that is to be studied is called the experimental region. A point in this region corresponds to a specific set of values of the inputs. Thus, an experimental design is a specification of points (runs) in the experimental region at which the response is to be computed.},
	pages = {145--200},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_5},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/37IWBFKN/Santner et al. - 2018 - Space-Filling Designs for Computer Experiments.pdf:application/pdf}
}

@incollection{santnerCriterionBasedExperimentalDesigns2018,
	location = {New York, {NY}},
	title = {Some Criterion-Based Experimental Designs},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_6},
	series = {Springer Series in Statistics},
	abstract = {Chapter 5 considered designs that attempt to spread observations “evenly” throughout the experimental region. Such designs were called space-filling designs. Recall that one rationale for using a space-filling design is the following. If it is believed that interesting features of the true model are just as likely to be in one part of the input region as another, observations should be taken in all portions of the input region. There are many heuristic criteria for producing designs that might be considered space-filling; several of these were discussed in Chap. 5. However none of the methods was tied to a statistical justification, and no single criterion was singled out as best.},
	pages = {201--246},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_6},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/SDE4DFN7/Santner et al. - 2018 - Some Criterion-Based Experimental Designs.pdf:application/pdf}
}

@incollection{santnerSensitivityAnalysisVariable2018,
	location = {New York, {NY}},
	title = {Sensitivity Analysis and Variable Screening},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_7},
	series = {Springer Series in Statistics},
	abstract = {This chapter discusses sensitivity analysis and the related topic of variable screening. The setup is as follows. A vector of inputs {\textbackslash}({\textbackslash}boldsymbol\{x\} = (x\_\{1\},{\textbackslash}ldots,x\_\{d\}){\textbackslash}) is given which potentially affects a “response” function {\textbackslash}(y({\textbackslash}boldsymbol\{x\}) = y(x\_\{1\},{\textbackslash}ldots,x\_\{d\}){\textbackslash}). Sensitivity analysis seeks to quantify how variation in {\textbackslash}(y({\textbackslash}boldsymbol\{x\}){\textbackslash}) can be apportioned to the inputs x1, …, xd and to the interactions among these inputs.},
	pages = {247--297},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_7},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/9X8TVERR/Santner et al. - 2018 - Sensitivity Analysis and Variable Screening.pdf:application/pdf}
}

@incollection{santnerCalibration2018,
	location = {New York, {NY}},
	title = {Calibration},
	isbn = {978-1-4939-8847-1},
	url = {https://doi.org/10.1007/978-1-4939-8847-1_8},
	series = {Springer Series in Statistics},
	abstract = {Ideally, every computer simulator should be calibrated using observations from the physical system that is modeled by the simulator. Roughly, calibration uses data from dual simulator and physical system platforms to estimate, with uncertainty, the unknown values of the calibration inputs that govern the physical system (and which can be set in the simulator).},
	pages = {299--379},
	booktitle = {The Design and Analysis of Computer Experiments},
	publisher = {Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	editor = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	urldate = {2020-09-15},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4939-8847-1_8},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/FZ6G59M3/Santner et al. - 2018 - Calibration.pdf:application/pdf}
}

@online{DesignAnalysisComputer,
	title = {Design and Analysis of Computer Experiments on {JSTOR}},
	url = {https://www.jstor.org/stable/2245858?seq=1#metadata_info_tab_contents},
	urldate = {2020-09-15},
	file = {Design and Analysis of Computer Experiments on JSTOR:/home/enitex/Zotero/storage/FLLXH48F/2245858.html:text/html}
}

@article{muehlenstaedtComputerExperimentsFunctional2017,
	title = {Computer experiments with functional inputs and scalar outputs by a norm-based approach},
	volume = {27},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-016-9672-z},
	doi = {10/ghbndr},
	abstract = {A framework for designing and analyzing computer experiments is presented, which is constructed for dealing with functional and scalar inputs and scalar outputs. For designing experiments with both functional and scalar inputs, a two-stage approach is suggested. The ﬁrst stage consists of constructing a candidate set for each functional input. During the second stage, an optimal combination of the found candidate sets and a Latin hypercube for the scalar inputs is sought. The resulting designs can be considered to be generalizations of Latin hypercubes. Gaussian process models are explored as metamodel. The functional inputs are incorporated into the Kriging model by applying norms in order to deﬁne distances between two functional inputs. We propose the use of B-splines to make the calculation of these norms computationally feasible.},
	pages = {1083--1097},
	number = {4},
	journaltitle = {Statistics and Computing},
	shortjournal = {Stat Comput},
	author = {Muehlenstaedt, Thomas and Fruth, Jana and Roustant, Olivier},
	urldate = {2020-09-15},
	date = {2017-07},
	langid = {english},
	file = {Muehlenstaedt et al. - 2017 - Computer experiments with functional inputs and sc.pdf:/home/enitex/Zotero/storage/LMBP2YJB/Muehlenstaedt et al. - 2017 - Computer experiments with functional inputs and sc.pdf:application/pdf}
}

@article{muehlenstaedtComputerExperimentsFunctional2017a,
	title = {Computer experiments with functional inputs and scalar outputs by a norm-based approach},
	volume = {27},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-016-9672-z},
	doi = {10/ghbndr},
	abstract = {A framework for designing and analyzing computer experiments is presented, which is constructed for dealing with functional and scalar inputs and scalar outputs. For designing experiments with both functional and scalar inputs, a two-stage approach is suggested. The first stage consists of constructing a candidate set for each functional input. During the second stage, an optimal combination of the found candidate sets and a Latin hypercube for the scalar inputs is sought. The resulting designs can be considered to be generalizations of Latin hypercubes. Gaussian process models are explored as metamodel. The functional inputs are incorporated into the Kriging model by applying norms in order to define distances between two functional inputs. We propose the use of B-splines to make the calculation of these norms computationally feasible.},
	pages = {1083--1097},
	number = {4},
	journaltitle = {Statistics and Computing},
	shortjournal = {Stat Comput},
	author = {Muehlenstaedt, Thomas and Fruth, Jana and Roustant, Olivier},
	urldate = {2020-09-15},
	date = {2017-07-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/CK9PISPW/Muehlenstaedt et al. - 2017 - Computer experiments with functional inputs and sc.pdf:application/pdf}
}

@article{morrisGaussianSurrogatesComputer2012,
	title = {Gaussian Surrogates for Computer Models With Time-Varying Inputs and Outputs},
	volume = {54},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.2012.648870},
	doi = {10/ghbnds},
	abstract = {Computer models of dynamic systems produce outputs that are functions of time; models that solve systems of differential equations often have this character. In many cases, time series output can be usefully reduced via principal components to simplify analysis. Time-indexed inputs, such as the functions that describe time-varying boundary conditions, are also common with such models. However, inputs that are functions of time often do not have one or a few “characteristic shapes” that are more common with output functions, and so, principal component representation has less potential for reducing the dimension of input functions. In this article, Gaussian process surrogates are described for models with inputs and outputs that are both functions of time. The focus is on construction of an appropriate covariance structure for such surrogates, some experimental design issues, and an application to a model of marrow cell dynamics.},
	pages = {42--50},
	number = {1},
	journaltitle = {Technometrics},
	author = {Morris, Max D.},
	urldate = {2020-09-15},
	date = {2012-02-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2012.648870},
	keywords = {Computer experiment, Dynamic model, Gaussian stochastic process, Maximin distance design, Meta-model.},
	file = {Full Text PDF:/home/enitex/Zotero/storage/M2C8IYD7/Morris - 2012 - Gaussian Surrogates for Computer Models With Time-.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/QYM53SYP/00401706.2012.html:text/html}
}

@article{fruthSequentialDesignsSensitivity2015,
	title = {Sequential designs for sensitivity analysis of functional inputs in computer experiments},
	volume = {134},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832014001732},
	doi = {10/f6w37d},
	abstract = {Computer experiments are nowadays commonly used to analyze industrial processes aiming at achieving a wanted outcome. Sensitivity analysis plays an important role in exploring the actual impact of adjustable parameters on the response variable. In this work we focus on sensitivity analysis of a scalar-valued output of a time-consuming computer code depending on scalar and functional input parameters. We investigate a sequential methodology, based on piecewise constant functions and sequential bifurcation, which is both economical and fully interpretable. The new approach is applied to a sheet metal forming problem in three sequential steps, resulting in new insights into the behavior of the forming process over time.},
	pages = {260--267},
	journaltitle = {Reliability Engineering \& System Safety},
	shortjournal = {Reliability Engineering \& System Safety},
	author = {Fruth, J. and Roustant, O. and Kuhnt, S.},
	urldate = {2020-09-15},
	date = {2015-02-01},
	langid = {english},
	keywords = {Functional input, Sensitivity analysis, Sequential approach, Sheet metal forming, Springback},
	file = {Fruth et al. - 2015 - Sequential designs for sensitivity analysis of fun.pdf:/home/enitex/Zotero/storage/JNFPJ48W/Fruth et al. - 2015 - Sequential designs for sensitivity analysis of fun.pdf:application/pdf}
}

@article{morrisDecomposingFunctionalModel2018,
	title = {Decomposing Functional Model Inputs for Variance-Based Sensitivity Analysis},
	volume = {6},
	url = {https://epubs-siam-org.eu1.proxy.openathens.net/doi/abs/10.1137/18M1173058},
	doi = {10/ghbnd7},
	abstract = {Variance-based sensitivity analysis is a popular technique for assessing the importance of model inputs when there are natural or meaningful probability distributions associated with each input. This approach can be used when some of the model inputs are functions rather than scalar valued, but may be somewhat less useful in this case because it does not address the nature of the relationships between functional inputs and model outputs. We consider the option of separating a random function-valued input, represented by a vector of relatively high dimension, into one or a few scalar-valued summaries that are suggested by the context of the modeling exercise, and an independent, high-dimensional “residual.” The first case we discuss is for inputs that are realizations of Gaussian processes, where the summary statistics are linear functionals of the input, and the residual can always be defined to be statistically independent of these. The second case is for input functions that might be described as “pulses” occurring in simulated time as a Poisson process, where the summary statistic is the number of such pulses and all other details form the residual. The third case involves periodic input functions for which the overall scale of the Fourier coefficients is controlled by the scalar-valued summary. We conclude by describing a graphical technique that may help to identify useful low-dimensional function summaries. When the model output is more sensitive to the low-dimensional summaries than to the residuals, this is useful information concerning the nature of model sensitivity, and also provides a route to constructing model surrogates with scalar-valued indices that accurately represent most of the variation in the output.},
	pages = {1584--1599},
	number = {4},
	journaltitle = {{SIAM}/{ASA} Journal on Uncertainty Quantification},
	shortjournal = {{SIAM}/{ASA} J. Uncertainty Quantification},
	author = {Morris, Max D.},
	urldate = {2020-09-15},
	date = {2018-01-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Full Text PDF:/home/enitex/Zotero/storage/WCMTC9UA/Morris - 2018 - Decomposing Functional Model Inputs for Variance-B.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/W7PLWQTU/18M1173058.html:text/html}
}

@article{morrisMaximinDistanceOptimal2014,
	title = {Maximin distance optimal designs for computer experiments with time-varying inputs and outputs},
	volume = {144},
	issn = {0378-3758},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375812002996},
	doi = {10/ghbnd8},
	series = {International Conference on Design of Experiments},
	abstract = {Computer models of dynamic systems produce outputs that are functions of time; models that solve systems of differential equations often have this character. Time-indexed inputs, such as the functions that describe time-varying boundary conditions, are also common with such models. Morris (2012) described a generalization of the Gaussian process often used to produce “meta-models” when inputs are finite-dimensional vectors, that can be used in the functional input setting, and showed how the maximin distance design optimality criterion (Johnson et al., 1990) can also be extended to this case. This paper describes an upper bound on the maximin distance criterion for functional inputs. A class of designs that are optimal under certain conditions is also presented; while these designs are of limited practical value, they show that the derived bound cannot be improved in the general case.},
	pages = {63--68},
	journaltitle = {Journal of Statistical Planning and Inference},
	shortjournal = {Journal of Statistical Planning and Inference},
	author = {Morris, Max D.},
	urldate = {2020-09-15},
	date = {2014-01-01},
	langid = {english},
	keywords = {Computer experiment, Dynamic model, Gaussian stochastic process, Maximin distance design, Meta-model, Surrogate},
	file = {Morris - 2014 - Maximin distance optimal designs for computer expe.pdf:/home/enitex/Morris - 2014 - Maximin distance optimal designs for computer expe.pdf:application/pdf}
}

@article{drigneiEmpiricalBayesianAnalysis2006,
	title = {Empirical Bayesian Analysis for High-Dimensional Computer Output},
	volume = {48},
	issn = {0040-1706},
	url = {https://doi.org/10.1198/004017005000000472},
	doi = {10/cvfj99},
	abstract = {This article proposes a two-stage statistical method for the analysis of multivariate computer experiments when at least one of the output dimensions is large. The stage-one data are modeled by a multivariate extension of a widely used scalar statistical model for computer output. Conditioned on stage-one data, a simple statistical model is then proposed for the stage-two data. The method is demonstrated in a geophysical application involving an ocean model.},
	pages = {230--240},
	number = {2},
	journaltitle = {Technometrics},
	author = {Drignei, Dorin},
	urldate = {2020-09-15},
	date = {2006-05-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/004017005000000472},
	keywords = {Computer experiment, Discrete-time Brownian bridge, Gaussian correlation, Geophysical model},
	file = {Full Text PDF:/home/enitex/Zotero/storage/9F4IXCT5/Drignei - 2006 - Empirical Bayesian Analysis for High-Dimensional C.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/MY4T98RC/004017005000000472.html:text/html}
}

@article{contiGaussianProcessEmulation2009,
	title = {Gaussian process emulation of dynamic computer codes},
	volume = {96},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/27798855},
	doi = {10/fsh9h2},
	abstract = {Computer codes are used in scientific research to study and predict the behaviour of complex systems. Their run times often make uncertainty and sensitivity analyses impractical because of the thousands of runs that are conventionally required, so efficient techniques have been developed based on a statistical representation of the code. The approach is less straightforward for dynamic codes, which represent time-evolving systems. We develop a novel iterative system to build a statistical model of dynamic computer codes, which is demonstrated on a rainfall-runoff simulator.},
	pages = {663--676},
	number = {3},
	journaltitle = {Biometrika},
	author = {{CONTI}, S. and {GOSLING}, J. P. and {OAKLEY}, J. E. and O'{HAGAN}, A.},
	urldate = {2020-09-15},
	date = {2009},
	note = {Publisher: [Oxford University Press, Biometrika Trust]}
}

@book{gramacySurrogatesGaussianProcess2020,
	location = {Boca Raton, Florida},
	title = {Surrogates: Gaussian process modeling, design and optimization for the applied sciences},
	publisher = {Chapman Hall/{CRC}},
	author = {Gramacy, Robert B.},
	date = {2020},
	file = {Gramacy - 2020 - Surrogates Gaussian process modeling, design and .pdf:/home/enitex/Gramacy - 2020 - Surrogates Gaussian process modeling, design and .pdf:application/pdf;Gramacy - 2020 - Surrogates Gaussian process modeling, design and .pdf:/home/enitex/Zotero/storage/QY4N4DRK/Gramacy - 2020 - Surrogates Gaussian process modeling, design and .pdf:application/pdf}
}

@article{janssenNewGenerationAgricultural2017,
	title = {Towards a new generation of agricultural system data, models and knowledge products: Information and communication technology},
	volume = {155},
	issn = {0308-521X},
	url = {http://www.sciencedirect.com/science/article/pii/S0308521X16305637},
	doi = {10/gbnmtk},
	shorttitle = {Towards a new generation of agricultural system data, models and knowledge products},
	abstract = {Agricultural modeling has long suffered from fragmentation in model implementation. Many models are developed, there is much redundancy, models are often poorly coupled, model component re-use is rare, and it is frequently difficult to apply models to generate real solutions for the agricultural sector. To improve this situation, we argue that an open, self-sustained, and committed community is required to co-develop agricultural models and associated data and tools as a common resource. Such a community can benefit from recent developments in information and communications technology ({ICT}). We examine how such developments can be leveraged to design and implement the next generation of data, models, and decision support tools for agricultural production systems. Our objective is to assess relevant technologies for their maturity, expected development, and potential to benefit the agricultural modeling community. The technologies considered encompass methods for collaborative development and for involving stakeholders and users in development in a transdisciplinary manner. Our qualitative evaluation suggests that as an overall research challenge, the interoperability of data sources, modular granular open models, reference data sets for applications and specific user requirements analysis methodologies need to be addressed to allow agricultural modeling to enter in the big data era. This will enable much higher analytical capacities and the integrated use of new data sources. Overall agricultural systems modeling needs to rapidly adopt and absorb state-of-the-art data and {ICT} technologies with a focus on the needs of beneficiaries and on facilitating those who develop applications of their models. This adoption requires the widespread uptake of a set of best practices as standard operating procedures.},
	pages = {200--212},
	journaltitle = {Agricultural Systems},
	shortjournal = {Agricultural Systems},
	author = {Janssen, Sander J. C. and Porter, Cheryl H. and Moore, Andrew D. and Athanasiadis, Ioannis N. and Foster, Ian and Jones, James W. and Antle, John M.},
	urldate = {2020-09-15},
	date = {2017-07-01},
	langid = {english},
	keywords = {Agricultural models, Big data, {ICT}, Linked data, Open science, Sensing, Visualization},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/3PS6WJ53/S0308521X16305637.html:text/html;ScienceDirect Full Text PDF:/home/enitex/Zotero/storage/YRR9AGDX/Janssen et al. - 2017 - Towards a new generation of agricultural system da.pdf:application/pdf}
}

@article{holzworthAPSIMEvolutionNew2014,
	title = {{APSIM} – Evolution towards a new generation of agricultural systems simulation},
	volume = {62},
	issn = {1364-8152},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815214002102},
	doi = {10/gddc8w},
	abstract = {Agricultural systems models worldwide are increasingly being used to explore options and solutions for the food security, climate change adaptation and mitigation and carbon trading problem domains. {APSIM} (Agricultural Production Systems {sIMulator}) is one such model that continues to be applied and adapted to this challenging research agenda. From its inception twenty years ago, {APSIM} has evolved into a framework containing many of the key models required to explore changes in agricultural landscapes with capability ranging from simulation of gene expression through to multi-field farms and beyond. Keating et al. (2003) described many of the fundamental attributes of {APSIM} in detail. Much has changed in the last decade, and the {APSIM} community has been exploring novel scientific domains and utilising software developments in social media, web and mobile applications to provide simulation tools adapted to new demands. This paper updates the earlier work by Keating et al. (2003) and chronicles the changing external challenges and opportunities being placed on {APSIM} during the last decade. It also explores and discusses how {APSIM} has been evolving to a “next generation” framework with improved features and capabilities that allow its use in many diverse topics.},
	pages = {327--350},
	journaltitle = {Environmental Modelling \& Software},
	shortjournal = {Environmental Modelling \& Software},
	author = {Holzworth, Dean P. and Huth, Neil I. and {deVoil}, Peter G. and Zurcher, Eric J. and Herrmann, Neville I. and {McLean}, Greg and Chenu, Karine and van Oosterom, Erik J. and Snow, Val and Murphy, Chris and Moore, Andrew D. and Brown, Hamish and Whish, Jeremy P. M. and Verrall, Shaun and Fainges, Justin and Bell, Lindsay W. and Peake, Allan S. and Poulton, Perry L. and Hochman, Zvi and Thorburn, Peter J. and Gaydon, Donald S. and Dalgliesh, Neal P. and Rodriguez, Daniel and Cox, Howard and Chapman, Scott and Doherty, Alastair and Teixeira, Edmar and Sharp, Joanna and Cichota, Rogerio and Vogeler, Iris and Li, Frank Y. and Wang, Enli and Hammer, Graeme L. and Robertson, Michael J. and Dimes, John P. and Whitbread, Anthony M. and Hunt, James and van Rees, Harm and {McClelland}, Tim and Carberry, Peter S. and Hargreaves, John N. G. and {MacLeod}, Neil and {McDonald}, Cam and Harsdorf, Justin and Wedgwood, Sara and Keating, Brian A.},
	urldate = {2020-09-15},
	date = {2014-12-01},
	langid = {english},
	keywords = {Agricultural systems, {APSIM}, Crop, Farming system, Gene-to-phenotype model, Model, Simulation},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/MZ4Q48MZ/S1364815214002102.html:text/html}
}

@article{janssenNewGenerationAgricultural2017a,
	title = {Towards a new generation of agricultural system data, models and knowledge products: Information and communication technology},
	volume = {155},
	issn = {0308-521X},
	url = {http://www.sciencedirect.com/science/article/pii/S0308521X16305637},
	doi = {10/gbnmtk},
	shorttitle = {Towards a new generation of agricultural system data, models and knowledge products},
	abstract = {Agricultural modeling has long suffered from fragmentation in model implementation. Many models are developed, there is much redundancy, models are often poorly coupled, model component re-use is rare, and it is frequently difficult to apply models to generate real solutions for the agricultural sector. To improve this situation, we argue that an open, self-sustained, and committed community is required to co-develop agricultural models and associated data and tools as a common resource. Such a community can benefit from recent developments in information and communications technology ({ICT}). We examine how such developments can be leveraged to design and implement the next generation of data, models, and decision support tools for agricultural production systems. Our objective is to assess relevant technologies for their maturity, expected development, and potential to benefit the agricultural modeling community. The technologies considered encompass methods for collaborative development and for involving stakeholders and users in development in a transdisciplinary manner. Our qualitative evaluation suggests that as an overall research challenge, the interoperability of data sources, modular granular open models, reference data sets for applications and specific user requirements analysis methodologies need to be addressed to allow agricultural modeling to enter in the big data era. This will enable much higher analytical capacities and the integrated use of new data sources. Overall agricultural systems modeling needs to rapidly adopt and absorb state-of-the-art data and {ICT} technologies with a focus on the needs of beneficiaries and on facilitating those who develop applications of their models. This adoption requires the widespread uptake of a set of best practices as standard operating procedures.},
	pages = {200--212},
	journaltitle = {Agricultural Systems},
	shortjournal = {Agricultural Systems},
	author = {Janssen, Sander J. C. and Porter, Cheryl H. and Moore, Andrew D. and Athanasiadis, Ioannis N. and Foster, Ian and Jones, James W. and Antle, John M.},
	urldate = {2020-09-15},
	date = {2017-07-01},
	langid = {english},
	keywords = {Agricultural models, Big data, {ICT}, Linked data, Open science, Sensing, Visualization},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/9X926UTT/S0308521X16305637.html:text/html;ScienceDirect Full Text PDF:/home/enitex/Zotero/storage/Y7Z2BUBZ/Janssen et al. - 2017 - Towards a new generation of agricultural system da.pdf:application/pdf}
}

@article{janssenNewGenerationAgricultural2017b,
	title = {Towards a new generation of agricultural system data, models and knowledge products: Information and communication technology},
	volume = {155},
	issn = {0308-521X},
	url = {http://www.sciencedirect.com/science/article/pii/S0308521X16305637},
	doi = {10/gbnmtk},
	shorttitle = {Towards a new generation of agricultural system data, models and knowledge products},
	abstract = {Agricultural modeling has long suffered from fragmentation in model implementation. Many models are developed, there is much redundancy, models are often poorly coupled, model component re-use is rare, and it is frequently difficult to apply models to generate real solutions for the agricultural sector. To improve this situation, we argue that an open, self-sustained, and committed community is required to co-develop agricultural models and associated data and tools as a common resource. Such a community can benefit from recent developments in information and communications technology ({ICT}). We examine how such developments can be leveraged to design and implement the next generation of data, models, and decision support tools for agricultural production systems. Our objective is to assess relevant technologies for their maturity, expected development, and potential to benefit the agricultural modeling community. The technologies considered encompass methods for collaborative development and for involving stakeholders and users in development in a transdisciplinary manner. Our qualitative evaluation suggests that as an overall research challenge, the interoperability of data sources, modular granular open models, reference data sets for applications and specific user requirements analysis methodologies need to be addressed to allow agricultural modeling to enter in the big data era. This will enable much higher analytical capacities and the integrated use of new data sources. Overall agricultural systems modeling needs to rapidly adopt and absorb state-of-the-art data and {ICT} technologies with a focus on the needs of beneficiaries and on facilitating those who develop applications of their models. This adoption requires the widespread uptake of a set of best practices as standard operating procedures.},
	pages = {200--212},
	journaltitle = {Agricultural Systems},
	shortjournal = {Agricultural Systems},
	author = {Janssen, Sander J. C. and Porter, Cheryl H. and Moore, Andrew D. and Athanasiadis, Ioannis N. and Foster, Ian and Jones, James W. and Antle, John M.},
	urldate = {2020-09-15},
	date = {2017-07-01},
	langid = {english},
	keywords = {Agricultural models, Big data, {ICT}, Linked data, Open science, Sensing, Visualization},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/RMBHJYET/S0308521X16305637.html:text/html;ScienceDirect Full Text PDF:/home/enitex/Zotero/storage/RERAEPU9/Janssen et al. - 2017 - Towards a new generation of agricultural system da.pdf:application/pdf}
}

@article{currinBayesianPredictionDeterministic1991,
	title = {Bayesian Prediction of Deterministic Functions, with Applications to the Design and Analysis of Computer Experiments},
	volume = {86},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1991.10475138},
	doi = {10/ggktqx},
	abstract = {This article is concerned with prediction of a function y(t) over a (multidimensional) domain T, given the function values at a set of “sites” t (1), t (2), …, t (n) in T, and with the design, that is, with the selection of those sites. The motivating application is the design and analysis of computer experiments, where t determines the input to a computer model of a physical or behavioral system, and y(t) is a response that is part of the output or is calculated from it. Following a Bayesian formulation, prior uncertainty about the function y is expressed by means of a random function Y, which is taken here to be a Gaussian stochastic process. The mean of the posterior process can be used as the prediction function ŷ(t), and the variance can be used as a measure of uncertainty. This kind of approach has been used previously in Bayesian interpolation and is strongly related to the kriging methods used in geostatistics. Here emphasis is placed on product linear and product cubic correlation functions, which yield prediction functions that are, respectively, linear or cubic splines in every dimension. A posterior entropy criterion is adopted for design; this minimizes the expected uncertainty about the posterior process, as measured by the entropy. A computational algorithm for finding entropy-optimal designs on multidimensional grids is described. Several examples are presented, including a two-dimensional experiment on a computer model of a thermal energy storage device and a six-dimensional experiment on an integrated circuit simulator. Predictions are made using several different families of correlation functions, with parameters chosen to maximize the likelihood. For comparison, predictions are also made via least squares fitting of various polynomial and spline models. The Bayesian design/prediction methods, which do not require any modeling of y, produce comparatively good predictions. For some correlation functions, however, the 95\% posterior probability intervals do not give adequate coverage of the true values of y at selected test sites. These methods are fairly simple and offer considerable potential for virtually automatic implementation, although further development is needed before they can be applied routinely in practice.},
	pages = {953--963},
	number = {416},
	journaltitle = {Journal of the American Statistical Association},
	author = {Currin, Carla and Mitchell, Toby and Morris, Max and Ylvisaker, Don},
	urldate = {2020-09-15},
	date = {1991-12-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1991.10475138},
	keywords = {Computer models, Correlation function, Cross-validation, Entropy, Experimental design, Interpolation, Kriging, Optimal design, Spline fitting, Stochastic processes},
	file = {Full Text PDF:/home/enitex/Zotero/storage/U4WYQUHY/Currin et al. - 1991 - Bayesian Prediction of Deterministic Functions, wi.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/CBIWADT6/01621459.1991.html:text/html}
}

@article{sacksDesignAnalysisComputer1989,
	title = {Design and Analysis of Computer Experiments},
	volume = {4},
	issn = {0883-4237, 2168-8745},
	url = {http://projecteuclid.org/euclid.ss/1177012413},
	doi = {10/fn276g},
	abstract = {Many scientific phenomena are now investigated by complex computer models or codes. A computer experiment is a number of runs of the code with various inputs. A feature of many computer experiments is that the output is deterministic--rerunning the code with the same inputs gives identical observations. Often, the codes are computationally expensive to run, and a common objective of an experiment is to fit a cheaper predictor of the output to the data. Our approach is to model the deterministic output as the realization of a stochastic process, thereby providing a statistical basis for designing experiments (choosing the inputs) for efficient prediction. With this model, estimates of uncertainty of predictions are also available. Recent work in this area is reviewed, a number of applications are discussed, and we demonstrate our methodology with an example.},
	pages = {409--423},
	number = {4},
	journaltitle = {Statistical Science},
	shortjournal = {Statist. Sci.},
	author = {Sacks, Jerome and Welch, William J. and Mitchell, Toby J. and Wynn, Henry P.},
	urldate = {2020-09-15},
	date = {1989-11},
	mrnumber = {MR1041765},
	zmnumber = {0955.62619},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Experimental design, computer-aided design, kriging, response surface, spatial statistics},
	file = {Full Text PDF:/home/enitex/Zotero/storage/3T963M3I/Sacks et al. - 1989 - Design and Analysis of Computer Experiments.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/YGHKPND3/1177012413.html:text/html}
}

@book{santnerDesignAnalysisComputer2018,
	location = {New York, {NY}},
	edition = {2nd ed. 2018},
	title = {The Design and Analysis of Computer Experiments},
	isbn = {978-1-4939-8847-1},
	series = {Springer Series in Statistics},
	abstract = {This book describes methods for designing and analyzing experiments that are conducted using a computer code, a computer experiment, and, when possible, a physical experiment. Computer experiments continue to increase in popularity as surrogates for and adjuncts to physical experiments. Since the publication of the first edition, there have been many methodological advances and software developments to implement these new methodologies. The computer experiments literature has emphasized the construction of algorithms for various data analysis tasks (design construction, prediction, sensitivity analysis, calibration among others), and the development of web-based repositories of designs for immediate application. While it is written at a level that is accessible to readers with Masters-level training in Statistics, the book is written in sufficient detail to be useful for practitioners and researchers. New to this revised and expanded edition: - An expanded presentation of basic material on computer experiments and Gaussian processes with additional simulations and examples - A new comparison of plug-in prediction methodologies for real-valued simulator output - An enlarged discussion of space-filling designs including Latin Hypercube designs ({LHDs}), near-orthogonal designs, and nonrectangular regions - A chapter length description of process-based designs for optimization, to improve good overall fit, quantile estimation, and Pareto optimization - A new chapter describing graphical and numerical sensitivity analysis tools - Substantial new material on calibration-based prediction and inference for calibration parameters - Lists of software that can be used to fit models discussed in the book to aid practitioners},
	pagetotal = {1},
	publisher = {Springer New York : Imprint: Springer},
	author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
	date = {2018},
	doi = {10.1007/978-1-4939-8847-1},
	keywords = {Applied mathematics, Engineering mathematics, Mathematical and Computational Engineering, Statistical Theory and Methods, Statistics, Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences},
	file = {Santner et al. - 2018 - The Design and Analysis of Computer Experiments.pdf:/home/enitex/Zotero/storage/XDI63R68/Santner et al. - 2018 - The Design and Analysis of Computer Experiments.pdf:application/pdf}
}

@article{gartonScorebasedLikelihoodRatios2020,
	title = {Score-based likelihood ratios and sparse Gaussian processes},
	url = {https://lib.dr.iastate.edu/etd/17938},
	doi = {10/ghbt2c},
	journaltitle = {Graduate Theses and Dissertations},
	author = {Garton, Nathaniel},
	date = {2020-01-01},
	file = {"Score-based likelihood ratios and sparse Gaussian processes" by Nathaniel Morrissey Garton:/home/enitex/Zotero/storage/J4TPWJ8V/17938.html:text/html;Full Text:/home/enitex/Zotero/storage/Y9WURCN3/Garton - 2020 - Score-based likelihood ratios and sparse Gaussian .pdf:application/pdf}
}

@article{gartonKnotSelectionSparse2020,
	title = {Knot selection in sparse Gaussian processes with a variational objective function},
	volume = {13},
	rights = {© 2020 The Authors. Statistical Analysis and Data Mining published by Wiley Periodicals {LLC}},
	issn = {1932-1872},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11459},
	doi = {10/ghbt2d},
	abstract = {Sparse, knot-based Gaussian processes have enjoyed considerable success as scalable approximations of full Gaussian processes. Certain sparse models can be derived through specific variational approximations to the true posterior, and knots can be selected to minimize the Kullback-Leibler divergence between the approximate and true posterior. While this has been a successful approach, simultaneous optimization of knots can be slow due to the number of parameters being optimized. Furthermore, there have been few proposed methods for selecting the number of knots, and no experimental results exist in the literature. We propose using a one-at-a-time knot selection algorithm based on Bayesian optimization to select the number and locations of knots. We showcase the competitive performance of this method relative to optimization of knots simultaneously on three benchmark datasets, but at a fraction of the computational cost.},
	pages = {324--336},
	number = {4},
	journaltitle = {Statistical Analysis and Data Mining: The {ASA} Data Science Journal},
	author = {Garton, Nathaniel and Niemi, Jarad and Carriquiry, Alicia},
	urldate = {2020-09-16},
	date = {2020},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11459},
	keywords = {knot selection, machine learning, nonparametric regression, sparse Gaussian processes, variational inference},
	file = {Snapshot:/home/enitex/Zotero/storage/KQFRWLVM/sam.html:text/html;Full Text PDF:/home/enitex/Zotero/storage/HH35NCDK/Garton et al. - 2020 - Knot selection in sparse Gaussian processes with a.pdf:application/pdf}
}

@article{golchiMonotoneEmulationComputer2015,
	title = {Monotone Emulation of Computer Experiments},
	volume = {3},
	url = {https://epubs-siam-org.eu1.proxy.openathens.net/doi/abs/10.1137/140976741},
	doi = {10/ghbzz4},
	abstract = {In statistical modeling of computer experiments, prior information is sometimes available about the underlying function. For example, the physical system simulated by the computer code may be known to be monotone with respect to some or all inputs. We develop a Bayesian approach to Gaussian process modeling capable of incorporating monotonicity information for computer model emulation. Markov chain Monte Carlo methods are used to sample from the posterior distribution of the process given the simulator output and monotonicity information. The performance of the proposed approach in terms of predictive accuracy and uncertainty quantification is demonstrated in a number of simulated examples as well as a real queuing system application.},
	pages = {370--392},
	number = {1},
	journaltitle = {{SIAM}/{ASA} Journal on Uncertainty Quantification},
	shortjournal = {{SIAM}/{ASA} J. Uncertainty Quantification},
	author = {Golchi, S. and Bingham, D. R. and Chipman, H. and Campbell, D. A.},
	urldate = {2020-09-18},
	date = {2015-01-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Full Text PDF:/home/enitex/Zotero/storage/4FXAT6NJ/Golchi et al. - 2015 - Monotone Emulation of Computer Experiments.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/YE99ZI2Q/140976741.html:text/html}
}

@article{golchiMonotoneFunctionEstimation2014,
	title = {Monotone Function Estimation for Computer Experiments},
	url = {http://arxiv.org/abs/1309.3802},
	abstract = {In statistical modeling of computer experiments sometimes prior information is available about the underlying function. For example, the physical system simulated by the computer code may be known to be monotone with respect to some or all inputs. We develop a Bayesian approach to Gaussian process modelling capable of incorporating monotonicity information for computer model emulation. Markov chain Monte Carlo methods are used to sample from the posterior distribution of the process given the simulator output and monotonicity information. The performance of the proposed approach in terms of predictive accuracy and uncertainty quantification is demonstrated in a number of simulated examples as well as a real queueing system application.},
	journaltitle = {{arXiv}:1309.3802 [stat]},
	author = {Golchi, Shirin and Bingham, Derek R. and Chipman, Hugh and Campbell, David A.},
	urldate = {2020-09-18},
	date = {2014-06-14},
	eprinttype = {arxiv},
	eprint = {1309.3802},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/enitex/Zotero/storage/ZJQZLJVR/Golchi et al. - 2014 - Monotone Function Estimation for Computer Experime.pdf:application/pdf;arXiv.org Snapshot:/home/enitex/Zotero/storage/DQ6MB3BH/1309.html:text/html}
}

@article{jonesEfficientGlobalOptimization1998,
	title = {Efficient Global Optimization of Expensive Black-Box Functions},
	volume = {13},
	issn = {1573-2916},
	url = {https://doi.org/10.1023/A:1008306431147},
	doi = {10/fg68nc},
	abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
	pages = {455--492},
	number = {4},
	journaltitle = {Journal of Global Optimization},
	shortjournal = {Journal of Global Optimization},
	author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
	urldate = {2020-09-18},
	date = {1998-12-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/enitex/Zotero/storage/ZZQ4AI3V/Jones et al. - 1998 - Efficient Global Optimization of Expensive Black-B.pdf:application/pdf}
}

@inproceedings{damianouDeepGaussianProcesses2013,
	title = {Deep Gaussian Processes},
	url = {http://proceedings.mlr.press/v31/damianou13a.html},
	abstract = {In this paper we introduce deep Gaussian process ({GP}) models. Deep {GPs} are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate {GP}. The inpu...},
	eventtitle = {Artificial Intelligence and Statistics},
	pages = {207--215},
	booktitle = {Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Damianou, Andreas and Lawrence, Neil},
	urldate = {2020-09-18},
	date = {2013-04-29},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	file = {damianou13a.pdf:/home/enitex/Zotero/storage/UVHHQIA3/damianou13a.pdf:application/pdf;Full Text PDF:/home/enitex/Zotero/storage/4L2HCJPW/Damianou and Lawrence - 2013 - Deep Gaussian Processes.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/TYNZ88A9/damianou13a.html:text/html}
}

@inproceedings{leeDeepNeuralNetworks2018,
	title = {Deep Neural Networks as Gaussian Processes},
	url = {https://openreview.net/forum?id=B1EA-M-0Z},
	abstract = {We show how to make predictions using deep networks, without training deep networks.},
	eventtitle = {International Conference on Learning Representations},
	author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	urldate = {2020-09-18},
	date = {2018-02-15},
	langid = {english},
	file = {Full Text PDF:/home/enitex/Zotero/storage/VYX6CVUC/Lee et al. - 2018 - Deep Neural Networks as Gaussian Processes.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/WZITDT3Q/forum.html:text/html}
}

@incollection{damianouVariationalGaussianProcess2011,
	title = {Variational Gaussian Process Dynamical Systems},
	url = {http://papers.nips.cc/paper/4330-variational-gaussian-process-dynamical-systems.pdf},
	pages = {2510--2518},
	booktitle = {Advances in Neural Information Processing Systems 24},
	publisher = {Curran Associates, Inc.},
	author = {Damianou, Andreas and Titsias, Michalis K. and Lawrence, Neil D.},
	editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
	urldate = {2020-09-18},
	date = {2011},
	file = {NIPS Full Text PDF:/home/enitex/Zotero/storage/WICEUH6A/Damianou et al. - 2011 - Variational Gaussian Process Dynamical Systems.pdf:application/pdf;NIPS Snapshot:/home/enitex/Zotero/storage/SHYK74AU/4330-variational-gaussian-process-dynamical-systems.html:text/html}
}

@incollection{rajaramDeepGaussianProcess2020,
	title = {Deep Gaussian Process Enabled Surrogate Models for Aerodynamic Flows},
	url = {https://arc.aiaa.org/doi/abs/10.2514/6.2020-1640},
	abstract = {Deep Gaussian process ({DGP}) models are multi-layered hierarchical generalizations of the well-known Gaussian process ({GP}) models widely used to construct surrogate models of aerodynamic quantities of interest. Combining the desirable features of {GP} models and deep neural networks ({DNN}), {DGP} models are known to perform well when training data is scarce and the behavior of the system response is highly non-stationary. In this paper, the performance of {DGP} models is evaluated against {GP} models. Detailed comparisons are made and conclusions are drawn in terms of training time, data requirements, predictive error, and robustness to choice of training design of experiments, among other metrics. Additionally, sensitivity and scalability analyses are conducted for the {GP} models to evaluate their usability. Finally, an adaptive construction of both models is presented, where the models are built sequentially by selecting points that maximize posterior variance. Several experiments are conducted with canonical test functions at varying input dimensions and a viscous transonic airfoil test case at 42 input dimensions. The experiments suggest that {DGP} models outperform traditional {GP} models in terms of accuracy but incur higher computational costs for training.},
	booktitle = {{AIAA} Scitech 2020 Forum},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Rajaram, Dushhyanth and Puranik, Tejas G. and Renganathan, Ashwin and Sung, Woong Je and Pinon-Fischer, Olivia J. and Mavris, Dimitri N. and Ramamurthy, Arun},
	urldate = {2020-09-19},
	date = {2020},
	doi = {10.2514/6.2020-1640},
	note = {\_eprint: https://arc.aiaa.org/doi/pdf/10.2514/6.2020-1640},
	file = {AIAA Snapshot:/home/enitex/Zotero/storage/KW57C6DP/6.html:text/html}
}

@article{radaidehSurrogateModelingAdvanced2020,
	title = {Surrogate modeling of advanced computer simulations using deep Gaussian processes},
	volume = {195},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832019301711},
	doi = {10/ghb47b},
	abstract = {The continuous advancements in computer power and computational modeling through high-fidelity and multiphysics simulations add more challenges on assessing their predictive capability. In this work, metamodeling or surrogate modeling through deep Gaussian processes ({DeepGP}) is performed to construct surrogates of advanced computer simulations drawn from the nuclear engineering area. This work is centered around three major ideas: (1) surrogate modeling through deep Gaussian processes ({DeepGP}), (2) simulation assessment through surrogate-based uncertainty quantification ({UQ}) methodologies, and (3) drawing conclusions regarding the underlying uncertainty of the four simulations considered in this paper. First, {DeepGP} models are trained, optimized, and validated to yield variety of features: (1) achieving high accuracy (small error metrics) on the validation set, (2) automatically capturing the surrogate model uncertainty (i.e. interpolation errors), (3) fitting multiple outputs with different scales simultaneously, (4) handling high dimensional input spaces, and (5) learning from small data amounts. Second, the validated {DeepGP} surrogates are utilized to efficiently perform {UQ} tasks such as uncertainty propagation (through Monte Carlo sampling), parameter screening (through Morris screening), and variance decomposition (through Sobol Indices) to investigate the selected simulations. Third, the thermal-hydraulics (fluid flow) results demonstrate the importance of inlet temperature uncertainty in void fraction predictions. For the reactor physics application (fuel depletion/consumption), {DeepGP} accurately captures the uncertainty in criticality calculations, which is about 0.6\% (i.e. a considerable value for this application). For the application of kinetic parameters (nuclear data), {DeepGP} successfully explains 95\% or more of the variance in all 12 outputs. Finally, {DeepGP}-based {UQ} analysis of the fuel performance application (materials science) shows the importance of the clad surface temperature, fuel porosity, and linear heat rate in explaining the variance of the maximum fuel centerline and surface temperatures.},
	pages = {106731},
	journaltitle = {Reliability Engineering \& System Safety},
	shortjournal = {Reliability Engineering \& System Safety},
	author = {Radaideh, Majdi I. and Kozlowski, Tomasz},
	urldate = {2020-09-19},
	date = {2020-03-01},
	langid = {english},
	keywords = {Uncertainty quantification, Bayesian learning, Deep {GP}, Gaussian processes, Nuclear reactor safety,, Nuclear simulations},
	file = {ScienceDirect Snapshot:/home/enitex/Zotero/storage/9WJ86WJ2/S0951832019301711.html:text/html;ScienceDirect Full Text PDF:/home/enitex/Zotero/storage/7ST4ZCEZ/Radaideh and Kozlowski - 2020 - Surrogate modeling of advanced computer simulation.pdf:application/pdf}
}

@inproceedings{dutordoirDeepGaussianProcess2017,
	title = {Deep Gaussian Process metamodeling of sequentially sampled non-stationary response surfaces},
	doi = {10/ghb47d},
	abstract = {Simulations are often used for the design of complex systems as they allow to explore the design space without the need to build several prototypes. Over the years, the simulation accuracy, as well as the associated computational cost has increased significantly, limiting the overall number of simulations during the design process. Therefore, metamodeling aims to approximate the simulation response with a cheap-to-evaluate mathematical approximation, learned from a limited set of simulator evaluations. Kernel-based methods using stationary kernels are nowadays wildly used. However, using stationary kernels for non-stationary responses can be inappropriate and result in poor models when combined with sequential design. We present the application of a novel kernel-based technique, known as Deep Gaussian Processes, which is better able to cope with these difficulties. We evaluate the method for non-stationary regression on a series of real-world problems, showing that it outperforms the standard Gaussian Processes with stationary kernels.},
	eventtitle = {2017 Winter Simulation Conference ({WSC})},
	pages = {1728--1739},
	booktitle = {2017 Winter Simulation Conference ({WSC})},
	author = {Dutordoir, Vincent and Knudde, Nicolas and van der Herten, Joachim and Couckuyt, Ivo and Dhaene, Tom},
	date = {2017-12},
	note = {{ISSN}: 1558-4305},
	keywords = {Gaussian processes, approximation theory, complex systems, Computational modeling, Deep Gaussian process metamodeling, Kernel, large-scale systems, mathematical approximation, Mathematical model, Metamodeling, nonstationary regression, regression analysis, response surface methodology, Response surface methodology, sequentially sampled non-stationary response surfaces, simulation, Simulations, stationary kernels, Training},
	file = {IEEE Xplore Abstract Record:/home/enitex/Zotero/storage/GSHSDR6L/8247911.html:text/html;Full Text:/home/enitex/Zotero/storage/5F2ULEYW/Dutordoir et al. - 2017 - Deep Gaussian Process metamodeling of sequentially.pdf:application/pdf}
}

@article{roseInvolvingStakeholdersAgricultural2018,
	title = {Involving stakeholders in agricultural decision support systems: Improving user-centred design},
	volume = {6},
	doi = {10.5836/ijam/2017-06-80},
	shorttitle = {Involving stakeholders in agricultural decision support systems},
	abstract = {Decision Support Systems ({DSS}) can improve farm management decisions and offer the opportunity to improve productivity and limit environmental degradation, both key tenets of the sustainable intensification of agriculture. While {DSS} are becoming increasingly useful for agriculture,
the uptake of computer-based support systems by farmers has remained disappointingly low as evidenced by studies spanning at least two decades. This paper explores the reasons behind this continued lack of interest. Is it, as previous researchers have proposed, the lack of user involvement
in the design and development of these systems? If so why should this be the case given decades of evidence underlining the value in user centred design ({UCD})? The paper reviews literature on the desirable characteristics of {DSS}, and then uses 78 interviews and five focus groups to explore
a case study of system use. The paper suggests that without changes to how systems are developed, particularly in how users are consulted, use of this technology will continue to be low. Practical suggestions are proposed to encourage more effective user-centred design. Chief amongst these,
the need for designers to undertake a ‘decision support context assessment’ before building and launching a product is highlighted. Better knowledge of user-centred design practices, a clear understanding of advice systems, and greater collaboration with human-computer interaction
researchers are also required.},
	pages = {80--89},
	number = {3},
	journaltitle = {International Journal of Agricultural Management},
	shortjournal = {International Journal of Agricultural Management},
	author = {{ROSE}, {DAVID} C. and {PARKER}, {CAROLINE} and {FODEY}, {JOE} and {PARK}, {CAROLINE} and {SUTHERLAND}, {WILLIAM} J. and {DICKS}, {LYNN} V.},
	date = {2018-01-01},
	keywords = {decision context assessment, decision support systems, decision support tools, participatory research, stakeholder engagement, technology use, user-centred design}
}

@article{roseInvolvingStakeholdersAgricultural2018a,
	title = {Involving stakeholders in agricultural decision support systems: Improving user-centred design},
	volume = {6},
	doi = {10.5836/ijam/2017-06-80},
	shorttitle = {Involving stakeholders in agricultural decision support systems},
	abstract = {Decision Support Systems ({DSS}) can improve farm management decisions and offer the opportunity to improve productivity and limit environmental degradation, both key tenets of the sustainable intensification of agriculture. While {DSS} are becoming increasingly useful for agriculture,
the uptake of computer-based support systems by farmers has remained disappointingly low as evidenced by studies spanning at least two decades. This paper explores the reasons behind this continued lack of interest. Is it, as previous researchers have proposed, the lack of user involvement
in the design and development of these systems? If so why should this be the case given decades of evidence underlining the value in user centred design ({UCD})? The paper reviews literature on the desirable characteristics of {DSS}, and then uses 78 interviews and five focus groups to explore
a case study of system use. The paper suggests that without changes to how systems are developed, particularly in how users are consulted, use of this technology will continue to be low. Practical suggestions are proposed to encourage more effective user-centred design. Chief amongst these,
the need for designers to undertake a ‘decision support context assessment’ before building and launching a product is highlighted. Better knowledge of user-centred design practices, a clear understanding of advice systems, and greater collaboration with human-computer interaction
researchers are also required.},
	pages = {80--89},
	number = {3},
	journaltitle = {International Journal of Agricultural Management},
	shortjournal = {International Journal of Agricultural Management},
	author = {{ROSE}, {DAVID} C. and {PARKER}, {CAROLINE} and {FODEY}, {JOE} and {PARK}, {CAROLINE} and {SUTHERLAND}, {WILLIAM} J. and {DICKS}, {LYNN} V.},
	date = {2018-01-01},
	keywords = {decision context assessment, decision support systems, decision support tools, participatory research, stakeholder engagement, technology use, user-centred design},
	file = {ROSE et al. - 2018 - Involving stakeholders in agricultural decision su.pdf:/home/enitex/Zotero/storage/7SMNJPME/ROSE et al. - 2018 - Involving stakeholders in agricultural decision su.pdf:application/pdf}
}

@incollection{mcconnellPrecisionConservationEnhance2018,
	title = {Precision Conservation to Enhance Wildlife Benefits in Agricultural Landscapes},
	rights = {© {ASA}, {CSSA}, and {SSSA}},
	isbn = {978-0-89118-356-3},
	url = {https://acsess.onlinelibrary.wiley.com/doi/abs/10.2134/agronmonogr59.c14},
	abstract = {Agriculture is the world's largest industry, continues to dominate human land use, and will become more intensive to meet global food demands associated with population growth. Sustainability of global agricultural systems will require strategic integration of conservation practices to protect ecosystems services, health, and productivity. Natural communities as a component of agricultural landscapes support wildlife populations that provide essential ecosystem services with broad societal value. However, allocation of land to noncrop uses entails economic opportunity costs to producers. Effective conservation delivery is dependent on being able to quantify and visualize both the expected costs and benefits. We argue that by identifying economic opportunities for conservation enrollment, increased adoption by landowners is achievable. Our primary goal was to illustrate the necessity, technology, and application of precision conservation in a wildlife management framework. The tools, technologies, and processes associated with precision agriculture can be adapted to inform conservation practice adoption when wildlife objectives are explicitly incorporated into farm- and landscape-level decision framework. We illustrate strategic, objective-driven conservation planning and delivery with case studies from an intensive agricultural landscape in the Lower Mississippi Alluvial Valley.},
	pages = {285--312},
	booktitle = {Precision Conservation: Geospatial Techniques for Agricultural and Natural Resources Conservation},
	publisher = {John Wiley \& Sons, Ltd},
	author = {{McConnell}, Mark D. and Burger, L. Wes},
	urldate = {2020-09-19},
	date = {2018},
	langid = {english},
	doi = {10.2134/agronmonogr59.c14},
	note = {Section: 14
\_eprint: https://acsess.onlinelibrary.wiley.com/doi/pdf/10.2134/agronmonogr59.c14},
	keywords = {Conservation Reserve Program, {CRP}, decision support tool, {DST}, Wetland Reserve Program, {WRP}},
	file = {Snapshot:/home/enitex/Zotero/storage/WWFL92SZ/agronmonogr59.html:text/html}
}

@article{maestriniDriversWithinfieldSpatial2018,
	title = {Drivers of within-field spatial and temporal variability of crop yield across the {US} Midwest},
	volume = {8},
	rights = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-32779-3},
	doi = {10/gfggt2},
	abstract = {Not all areas of a farmer’s field are equal; some always produce more relative to the rest of the field, others always less, while still other areas fluctuate in their production capacity from one year to the next, depending on the interaction between climate, soil, topography and management. Understanding why the yield in certain portions of a field has a high variability over time—we call these areas unstable—is of paramount importance both from an economic and an environmental point of view, as it is through the better management of these areas that we can improve yields or reduce input costs and environmental impact. In this research, we analyzed data from 338 fields cultivated with maize, soybean, wheat and cotton in the {US} Midwest to understand how topographic attributes and rain affect yield stability over time. In addition to this high resolution yield monitor dataset, we used publicly available data on topography, rain and soil information to test the hypothesis that within-field areas characterized by a low topographic wetness index (proxy for areas with probability of lower water content) always perform poorly (low and stable yield) compared to the rest of the field because they are drier, and that areas of a field characterized by a mid-high wetness index (high and stable yield) always perform well relative to rest of the field because they have greater water availability to plants. The relative performance of areas of a field with a very high wetness index (e.g. depressions) strongly depends on rain patterns because they may be waterlogged in wet years, yielding less than the rest of the field, or wetter during dry years, yielding more than the rest of the field. We present three different observations from this dataset to support our hypothesis. First, we show that the average topographic wetness index in the different stability zones is lower in low and stable yield areas, high in high and stable yield areas and even higher in unstable yield areas (p {\textless} 0.05). Second, we show that in dry years (low precipitation at plant emergence or in July), unstable zones perform relatively better compared to the rest of the field. Third, we show that temporal yield variability is positively correlated (p {\textless} 0.05) with the probability of observing gleying processes associated with waterlogging for part of the year. These findings shed light on mechanisms underlying temporal variability of yield and can help guide management solutions to increase profit and improve environmental quality.},
	pages = {14833},
	number = {1},
	journaltitle = {Scientific Reports},
	author = {Maestrini, Bernardo and Basso, Bruno},
	urldate = {2020-09-19},
	date = {2018-10-04},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/home/enitex/Zotero/storage/62DBZLPJ/Maestrini and Basso - 2018 - Drivers of within-field spatial and temporal varia.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/E7IARXGU/s41598-018-32779-3.html:text/html}
}

@article{brandesSubfieldProfitabilityAnalysis2016,
	title = {Subfield profitability analysis reveals an economic case for cropland diversification},
	volume = {11},
	issn = {1748-9326},
	url = {https://doi.org/10.1088%2F1748-9326%2F11%2F1%2F014009},
	doi = {10/ghb6kp},
	abstract = {Public agencies and private enterprises increasingly desire to achieve ecosystem service outcomes in agricultural systems, but are limited by perceived conflicts between economic and ecosystem service goals and a lack of tools enabling effective operational management. Here we use Iowa—an agriculturally homogeneous state representative of the Maize Belt—to demonstrate an economic rationale for cropland diversification at the subfield scale. We used a novel computational framework that integrates disparate but publicly available data to map ∼3.3 million unique potential management polygons (9.3 Mha) and reveal subfield opportunities to increase overall field profitability. We analyzed subfield profitability for maize/soybean fields during 2010–2013—four of the most profitable years in recent history—and projected results for 2015. While cropland operating at a loss of {US}\$ 250 ha−1 or more was negligible between 2010 and 2013 at 18 000–190 000 ha ({\textless}2\% of row-crop land), the extent of highly unprofitable land increased to 2.5 Mha, or 27\% of row-crop land, in the 2015 projection. Aggregation of these areas to the township level revealed ‘hotspots’ for potential management change in Western, Central, and Northeast Iowa. In these least profitable areas, incorporating conservation management that breaks even (e.g., planting low-input perennials), into low-yielding portions of fields could increase overall cropland profitability by 80\%. This approach is applicable to the broader region and differs substantially from the status quo of ‘top-down’ land management for conservation by harnessing private interest to align profitability with the production of ecosystem services.},
	pages = {014009},
	number = {1},
	journaltitle = {Environmental Research Letters},
	shortjournal = {Environ. Res. Lett.},
	author = {Brandes, E. and {McNunn}, G. S. and Schulte, L. A. and Bonner, I. J. and Muth, D. J. and Babcock, B. A. and Sharma, B. and Heaton, E. A.},
	urldate = {2020-09-19},
	date = {2016-01},
	langid = {english},
	note = {Publisher: {IOP} Publishing},
	file = {IOP Full Text PDF:/home/enitex/Zotero/storage/CFHKJC7X/Brandes et al. - 2016 - Subfield profitability analysis reveals an economi.pdf:application/pdf}
}

@article{schulteAgroecosystemRestorationStrategic2006,
	title = {Agroecosystem restoration through strategic integration of perennials},
	volume = {61},
	issn = {0022-4561},
	url = {https://link.gale.com/apps/doc/A157081816/AONE?u=iastu_main&sid=zotero&xid=d80b0a93},
	pages = {164A--},
	number = {6},
	journaltitle = {Journal of Soil and Water Conservation},
	author = {Schulte, Lisa A. and Liebman, Matt and Asbjornsen, Heidi and Crow, Thomas R.},
	urldate = {2020-09-19},
	date = {2006},
	keywords = {Agroecosystems, Annual plants, Landscape protection, Perennial plants, ⛔ No {DOI} found}
}

@article{schultePrairieStripsImprove2017,
	title = {Prairie strips improve biodiversity and the delivery of multiple ecosystem services from corn–soybean croplands},
	volume = {114},
	rights = {©  . Freely available online through the {PNAS} open access option.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/114/42/11247},
	doi = {10/gcg3f4},
	abstract = {Loss of biodiversity and degradation of ecosystem services from agricultural lands remain important challenges in the United States despite decades of spending on natural resource management. To date, conservation investment has emphasized engineering practices or vegetative strategies centered on monocultural plantings of nonnative plants, largely excluding native species from cropland. In a catchment-scale experiment, we quantified the multiple effects of integrating strips of native prairie species amid corn and soybean crops, with prairie strips arranged to arrest run-off on slopes. Replacing 10\% of cropland with prairie strips increased biodiversity and ecosystem services with minimal impacts on crop production. Compared with catchments containing only crops, integrating prairie strips into cropland led to greater catchment-level insect taxa richness (2.6-fold), pollinator abundance (3.5-fold), native bird species richness (2.1-fold), and abundance of bird species of greatest conservation need (2.1-fold). Use of prairie strips also reduced total water runoff from catchments by 37\%, resulting in retention of 20 times more soil and 4.3 times more phosphorus. Corn and soybean yields for catchments with prairie strips decreased only by the amount of the area taken out of crop production. Social survey results indicated demand among both farming and nonfarming populations for the environmental outcomes produced by prairie strips. If federal and state policies were aligned to promote prairie strips, the practice would be applicable to 3.9 million ha of cropland in Iowa alone.},
	pages = {11247--11252},
	number = {42},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Schulte, Lisa A. and Niemi, Jarad and Helmers, Matthew J. and Liebman, Matt and Arbuckle, J. Gordon and James, David E. and Kolka, Randall K. and O’Neal, Matthew E. and Tomer, Mark D. and Tyndall, John C. and Asbjornsen, Heidi and Drobney, Pauline and Neal, Jeri and Ryswyk, Gary Van and Witte, Chris},
	urldate = {2020-09-19},
	date = {2017-10-17},
	langid = {english},
	pmid = {28973922},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {agriculture, agroecosystem services, perennials, sustainability, {US} Corn Belt},
	file = {Full Text PDF:/home/enitex/Zotero/storage/CSFPL4VI/Schulte et al. - 2017 - Prairie strips improve biodiversity and the delive.pdf:application/pdf;Snapshot:/home/enitex/Zotero/storage/X6VR88BH/11247.html:text/html}
}

@online{SwitchgrassNitrogenResponse,
	title = {Switchgrass nitrogen response and estimated production costs on diverse sites - {ProQuest}},
	url = {https://search.proquest.com/docview/1939769717?accountid=10906&rfr_id=info%3Axri%2Fsid%3Aprimo},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the {ProQuest} Platform.},
	urldate = {2020-09-19},
	langid = {english},
	file = {Snapshot:/home/enitex/Zotero/storage/GX6H24RC/1939769717.html:text/html}
}

@article{fikeSwitchgrassNitrogenResponse2017,
	title = {Switchgrass nitrogen response and estimated production costs on diverse sites},
	volume = {9},
	url = {https://lib.dr.iastate.edu/agron_pubs/352},
	doi = {10/gbzjd4},
	pages = {1526--1542},
	number = {10},
	journaltitle = {Global Change Biology Bioenergy},
	author = {Fike, John and Pease, James and Owens, Vance and Farris, Rodney and Hansen, Julie and Heaton, Emily and Hong, Chang and Mayton, Hilary and Mitchell, Robert and Viands, Donald},
	date = {2017-10-01},
	file = {"Switchgrass nitrogen response and estimated production costs on divers" by John H. Fike, James W. Pease et al.:/home/enitex/Zotero/storage/CX3YZZZY/352.html:text/html}
}

@book{rasmussenGaussianProcessesMachine2006,
	location = {Cambridge, Mass},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	series = {Adaptive computation and machine learning},
	pagetotal = {248},
	publisher = {{MIT} Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	date = {2006},
	note = {{OCLC}: ocm61285753},
	keywords = {Gaussian processes, Data processing, Machine learning, Mathematical models},
	file = {Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf:/home/enitex/Zotero/storage/Q53UG5QS/Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf:application/pdf}
}